{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero and Few shot comparison between a standard GPT2 model and the same model after been fineTuned with 90k matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "#Standard GPT2: Tokenizer and model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Fine-tuned GPT2: Tokenizer1 and model1\n",
    "\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"ChessBOT_GPT2/Model90K/model\")\n",
    "if tokenizer1.pad_token is None:\n",
    "    tokenizer1.pad_token = tokenizer1.eos_token\n",
    "\n",
    "model1 = GPT2LMHeadModel.from_pretrained(\"ChessBOT_GPT2/Model90K/model\", trust_remote_co4de=True)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,prompt, max_new_tokens=100, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    prompt_length = len(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],  \n",
    "            max_length=prompt_length + max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    new_text = generated_text[len(tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)):]\n",
    "    \n",
    "    return new_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "## if starting_fen is None, the board will be initialized to the starting position \n",
    "def get_fen_from_uci_sequence(uci_sequence, starting_fen=None):\n",
    "\n",
    "    \n",
    "    if starting_fen:\n",
    "        board = chess.Board(starting_fen)\n",
    "    else:\n",
    "        board = chess.Board()  \n",
    "    \n",
    "    moves = uci_sequence.strip().split()\n",
    "    \n",
    "    for move_str in moves:\n",
    "        \n",
    "        try:\n",
    "            move = chess.Move.from_uci(move_str)\n",
    "            \n",
    "            if move in board.legal_moves:\n",
    "                board.push(move)\n",
    "\n",
    "            else:\n",
    "                print(f\"Moves {move_str} not legal!\")\n",
    "                print(\"Moves sequence was interrupted.\")\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(f\"Moves {move_str} not in UCI format!\")\n",
    "            print(\"Moves sequence was interrupted.\")\n",
    "            break\n",
    "    \n",
    "    # Stampa la scacchiera nello stato attuale\n",
    "    print(\"\\nState of the board:\")\n",
    "    print(board)\n",
    "    \n",
    "    # Ritorna la FEN finale\n",
    "    final_fen = board.fen()\n",
    "    print(f\"\\nFEN: {final_fen}\")\n",
    "    \n",
    "    return final_fen, board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status\n",
    "Selection of the Hystory of the game from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State of the board:\n",
      "r n . q k . . r\n",
      "p p . . p p B p\n",
      ". . . p . n p .\n",
      ". . p . . . . .\n",
      ". . P N . . b .\n",
      ". . . P . . . .\n",
      "P P . . P P P P\n",
      "R . . Q K B N R\n",
      "\n",
      "FEN: rn1qk2r/pp2ppBp/3p1np1/2p5/2PN2b1/3P4/PP2PPPP/R2QKBNR b KQkq - 2 7\n"
     ]
    }
   ],
   "source": [
    "movesHistory=\"c2c4 c7c5 b1a3 g8f6 d2d3 d7d6 a3b5 g7g6 c1h6 f8g7 h6g7 c8g4 b5d4\"\n",
    "\n",
    "FEN,board=get_fen_from_uci_sequence(movesHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f\"\"\"You are a chess assistant. Given a sequence of UCI moves, suggest two reasonable next moves for the side to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "\n",
    "Moves so far:\n",
    "{movesHistory}\n",
    "\n",
    "Your suggestions:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "1. Black has to play with some movement.\n",
      "\n",
      "2. White can play with some movement.\n",
      "\n",
      "3. White has to play with movement.\n",
      "\n",
      "4. White can play with movement.\n",
      "\n",
      "5. White can\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "d1d2 d8d7 d7a4 a4a7 a7a5 d2b4 f6e4 f1d3 e4d6 e1e2 g4d7 h1c1 d6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,prompt1,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,prompt1,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of the prompt asking for 2 possibile moves got completely ignored from both the models, the finetuned one simply start try playing for both the player.\n",
    "The slice of the prompt: \"Remeber the first moves of the sequence I gave is played by white, the second by black and so on\" is crucial for the second model otherwise the response often start with a moves of the wrong player\n",
    "\n",
    "The difference between the two model is clear stdGPT2 doesn't get what is a UCI format by itself and as a single move show some random char, on the other hand the other model seem not to understand anymore the request of having two suggestion nor having a suggestion fot the next move as it start simply to play.\n",
    "\n",
    "Sometimes the finetuned model start the response with a single char followed by a space and then a sequence of UCI moves, if the single CHAR is ignored the following move is usually valid\n",
    "\n",
    "\n",
    "\n",
    "Here a longer moves History to see if the model can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State of the board:\n",
      ". . . . . k . .\n",
      ". p . R . p p p\n",
      "p . . b . . . .\n",
      ". . . . . . Q .\n",
      ". . . . . P . .\n",
      ". . . . . . . P\n",
      "P . . . . K P .\n",
      ". . . q . . . .\n",
      "\n",
      "FEN: 5k2/1p1R1ppp/p2b4/6Q1/5P2/7P/P4KP1/3q4 b - - 27 43\n"
     ]
    }
   ],
   "source": [
    "movesHistoryLong=\"d2d4 d7d5 c2c4 e7e6 b1c3 c7c5 c4d5 e6d5 g1f3 g8f6 c1g5 c5d4 f3d4 f8e7 e2e3 b8c6 f1e2 e8g8 e1g1 a7a6 a1c1 f8e8 e2f3 c8e6 c3e2 c6e5 e2f4 e5f3 d1f3 e6g4 f3g3 d8d7 g5f6 e7f6 h2h3 g4f5 d4f5 d7f5 c1c5 a8d8 f1d1 f6b2 c5d5 d8d5 f4d5 f5c2 d5e7 g8f8 d1d7 c2c5 e7d5 b2e5 f2f4 e5d6 g3g5 e8e3 d5e3 c5e3 g1f1 e3d3 f1f2 d3d4 f2f3 d4d1 f3f2 d1d2 f2f1 d2d1 f1f2 d1d2 f2f1 d2d3 f1f2 d3d2 f2f1 d2d3 f1f2 d3d2 f2f1 d2d3 f1f2 d3d4 f2f3 d4d1 f3f2\"\n",
    "\n",
    "FEN,board=get_fen_from_uci_sequence(movesHistoryLong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptL = f\"\"\"You are a chess assistant. Given a sequence of UCI moves, suggest two reasonable next moves for the side to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "\n",
    "Moves so far:\n",
    "{movesHistoryLong}\n",
    "\n",
    "Your suggestions:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "This is the most reasonable move, given the sequence I gave.\n",
      "\n",
      "The last move is played by white, the second by black and so on.\n",
      "\n",
      "Remeber the first moves of the sequence I gave is played by white\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "1 f2d3 f1 f1f2 d3d2 f2f3 d1f2 d3d4 f3d2 f2 d7d5 f2 f2f3 d3d2 f1 f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,promptL,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,promptL,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already with an history not huge compared to the one in the dataset also the fineTuned model start messing up proposing unlegal moves,sometimes moving a piece like it was another, and also start showing \"UCI moves\" of 2 char; that probably caused by the limited context GPT2 can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now if some FewShot could improve the performance especially of stdGPT2 which didn't bring any useful prediction with ZeroShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt2=f\"\"\"\n",
    "You are a chess assistant. Given a sequence of UCI moves, suggest two plausible next moves for the player to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "    \n",
    "Example 1  \n",
    "Moves so far:  \n",
    "d2d4 g8f6 c2c4 e7e6 g1f3 d7d5 b1c3 f8e7 c1g5 h7h6  \n",
    "Suggestions: g5h4 e2e3  \n",
    "\n",
    "Example 2  \n",
    "Moves so far:  \n",
    "e2e4 e7e5 g1f3 b8c6 f1b5 a7a6 b5a4 g8f6 e1g1 f8e7 f1e1 b7b5 a4b3  \n",
    "Suggestions: c2c3 f3g5  \n",
    "\n",
    "Now it's your turn:\n",
    "\n",
    "Moves so far:  \n",
    "{movesHistory}  \n",
    "Suggestions: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "Example 3  \n",
      "\n",
      "Moves so far:  \n",
      "\n",
      "e2e4 e7e5  \n",
      "\n",
      "Ditto.\n",
      "\n",
      "Moves so far:  \n",
      "\n",
      "e2e4 e7e\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "1c1 d8e8 g7f5 f5e4 d3e2 e2a6 a2a4 e4f5 g4f3 g6f5 e7d7 a6b7 b7c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,prompt2,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,prompt2,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems seen with GPT2 stay the same, also the thing it say are not coherent with the movesHistory: \"c2c4 c7c5 b1a3 g8f6 d2d3 d7d6 a3b5 g7g6 c1h6 f8g7 h6g7 c8g4 b5d4\"\n",
    "\n",
    "The fineTuned model here doesn't improve in any way and more often start the sequence with a 3 char which doesn't represent any moves in the UCI format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import re\n",
    "import os\n",
    "\n",
    "def evaluate_legal_moves_from_file(filename, model, tokenizer, num_samples=10000, temperature=0.7, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculates statistics on legal move predictions from the model.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to the file with move sequences, one per line\n",
    "        model: Language model to evaluate\n",
    "        tokenizer: Tokenizer associated with the model\n",
    "        num_samples (int): Maximum number of samples to evaluate\n",
    "        temperature (float): Temperature to use for generation\n",
    "        verbose (bool): If True, prints details during execution\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with prediction statistics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File {filename} not found\")\n",
    "    \n",
    "    # Read move sequences from file\n",
    "    with open(filename, 'r') as f:\n",
    "        move_sequences = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Limit the number of samples if necessary\n",
    "    if num_samples and num_samples < len(move_sequences):\n",
    "        move_sequences = move_sequences[:num_samples]\n",
    "    \n",
    "    # Statistics\n",
    "    total_predictions = 0\n",
    "    legal_moves = 0\n",
    "    illegal_moves = 0\n",
    "    invalid_format = 0\n",
    "    stats_by_length = {}\n",
    "    \n",
    "    # Pattern to extract the first UCI move from the model's response\n",
    "    move_pattern = r'([a-h][1-8][a-h][1-8][qrbnkQRBNK]?)'\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluating {len(move_sequences)} move sequences...\")\n",
    "    \n",
    "    # Process each sequence\n",
    "    for i, sequence in enumerate(move_sequences):\n",
    "        if verbose and i % 10 == 0:\n",
    "            print(f\"Processing sequence {i+1}/{len(move_sequences)}...\")\n",
    "        \n",
    "        # Set up the chessboard with the sequence moves\n",
    "        board = chess.Board()\n",
    "        moves = sequence.split()\n",
    "        \n",
    "        # Apply all moves to the board\n",
    "        valid_sequence = True\n",
    "        for move_str in moves:\n",
    "            try:\n",
    "                move = chess.Move.from_uci(move_str)\n",
    "                if move in board.legal_moves:\n",
    "                    board.push(move)\n",
    "                else:\n",
    "                    valid_sequence = False\n",
    "                    if verbose:\n",
    "                        print(f\"Invalid move in sequence {i+1}: {move_str}\")\n",
    "                    break\n",
    "            except ValueError:\n",
    "                valid_sequence = False\n",
    "                if verbose:\n",
    "                    print(f\"Invalid move format in sequence {i+1}: {move_str}\")\n",
    "                break\n",
    "        \n",
    "        # Skip invalid sequences\n",
    "        if not valid_sequence:\n",
    "            continue\n",
    "        \n",
    "        # Generate the prompt for the model\n",
    "        prompt = f\"\"\"You are a chess assistant. Given a sequence of UCI moves, suggest two reasonable next moves for the side to play.\n",
    "        Remember the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "\n",
    "        Moves so far:\n",
    "        {sequence}\n",
    "\n",
    "        Your suggestions:\"\"\"\n",
    "        \n",
    "        # Generate the prediction\n",
    "        prediction = generate_text(model, tokenizer, prompt, max_new_tokens=10, temperature=temperature)\n",
    "        \n",
    "        # Extract the first move in UCI format from the prediction\n",
    "        match = re.search(move_pattern, prediction)\n",
    "        predicted_move = match.group(1) if match else None\n",
    "        \n",
    "        # Track statistics by sequence length\n",
    "        seq_length = len(moves)\n",
    "        if seq_length not in stats_by_length:\n",
    "            stats_by_length[seq_length] = {\n",
    "                'total': 0, 'legal': 0, 'illegal': 0, 'invalid': 0\n",
    "            }\n",
    "        \n",
    "        # Check if the predicted move is valid\n",
    "        total_predictions += 1\n",
    "        stats_by_length[seq_length]['total'] += 1\n",
    "        \n",
    "        if predicted_move is None:\n",
    "            invalid_format += 1\n",
    "            stats_by_length[seq_length]['invalid'] += 1\n",
    "            if verbose:\n",
    "                print(f\"Prediction without valid UCI format: '{prediction}'\")\n",
    "        else:\n",
    "            try:\n",
    "                move = chess.Move.from_uci(predicted_move)\n",
    "                if move in board.legal_moves:\n",
    "                    legal_moves += 1\n",
    "                    stats_by_length[seq_length]['legal'] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"Legal move predicted: {predicted_move}\")\n",
    "                else:\n",
    "                    illegal_moves += 1\n",
    "                    stats_by_length[seq_length]['illegal'] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"Illegal move predicted: {predicted_move}\")\n",
    "            except ValueError:\n",
    "                invalid_format += 1\n",
    "                stats_by_length[seq_length]['invalid'] += 1\n",
    "                if verbose:\n",
    "                    print(f\"Invalid predicted move format: {predicted_move}\")\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    legal_rate = legal_moves / total_predictions * 100 if total_predictions > 0 else 0\n",
    "    illegal_rate = illegal_moves / total_predictions * 100 if total_predictions > 0 else 0\n",
    "    invalid_rate = invalid_format / total_predictions * 100 if total_predictions > 0 else 0\n",
    "    \n",
    "    # Results\n",
    "    results = {\n",
    "        'total_predictions': total_predictions,\n",
    "        'legal_moves': legal_moves,\n",
    "        'illegal_moves': illegal_moves,\n",
    "        'invalid_format': invalid_format,\n",
    "        'legal_rate': legal_rate,\n",
    "        'illegal_rate': illegal_rate,\n",
    "        'invalid_rate': invalid_rate,\n",
    "        'stats_by_length': stats_by_length\n",
    "    }\n",
    "    \n",
    "    # Print statistics report\n",
    "    print(\"\\n--- PREDICTION STATISTICS ---\")\n",
    "    print(f\"Total predictions: {total_predictions}\")\n",
    "    print(f\"Legal moves: {legal_moves} ({legal_rate:.2f}%)\")\n",
    "    print(f\"Illegal moves: {illegal_moves} ({illegal_rate:.2f}%)\")\n",
    "    print(f\"Invalid format: {invalid_format} ({invalid_rate:.2f}%)\")\n",
    "        \n",
    "    print(\"\\n--- DETAIL BY SEQUENCE LENGTH ---\")\n",
    "    for length, stats in sorted(stats_by_length.items()):\n",
    "        if stats['total'] > 0:\n",
    "            legal_pct = stats['legal'] / stats['total'] * 100\n",
    "            print(f\"Length {length}: {stats['legal']}/{stats['total']} legal moves ({legal_pct:.2f}%)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"testing_sequence.txt\" has been generated from 10k matches not used in the fineTuning of the model, it contain the first moves for each games: the lenght of each sequence has been randomically chosen by random.randint between 1 and 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PREDICTION STATISTICS ---\n",
      "Total predictions: 10000\n",
      "Legal moves: 3274 (32.74%)\n",
      "Illegal moves: 6726 (67.26%)\n",
      "Invalid format: 0 (0.00%)\n",
      "\n",
      "--- DETAIL BY SEQUENCE LENGTH ---\n",
      "Length 1: 261/462 legal moves (56.49%)\n",
      "Length 2: 143/504 legal moves (28.37%)\n",
      "Length 3: 180/506 legal moves (35.57%)\n",
      "Length 4: 182/507 legal moves (35.90%)\n",
      "Length 5: 160/557 legal moves (28.73%)\n",
      "Length 6: 123/491 legal moves (25.05%)\n",
      "Length 7: 137/503 legal moves (27.24%)\n",
      "Length 8: 113/469 legal moves (24.09%)\n",
      "Length 9: 199/498 legal moves (39.96%)\n",
      "Length 10: 105/472 legal moves (22.25%)\n",
      "Length 11: 190/493 legal moves (38.54%)\n",
      "Length 12: 101/484 legal moves (20.87%)\n",
      "Length 13: 237/463 legal moves (51.19%)\n",
      "Length 14: 114/498 legal moves (22.89%)\n",
      "Length 15: 223/494 legal moves (45.14%)\n",
      "Length 16: 132/520 legal moves (25.38%)\n",
      "Length 17: 199/532 legal moves (37.41%)\n",
      "Length 18: 137/505 legal moves (27.13%)\n",
      "Length 19: 214/513 legal moves (41.72%)\n",
      "Length 20: 124/529 legal moves (23.44%)\n",
      "Percentage of legal moves: 32.74%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = evaluate_legal_moves_from_file(\n",
    "    \"testing_sequences.txt\", \n",
    "    model1,                 \n",
    "    tokenizer1,             \n",
    "    temperature=0.7,      \n",
    "    num_samples=10000, #The 10k squences are in the file were not used in the tuning\n",
    "    verbose=False         \n",
    ")\n",
    "\n",
    "print(f\"Percentage of legal moves: {results['legal_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all these information so they can be retriven if the notebook encount some issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"legalRate.txt\", 'w') as f:\n",
    "        # Write overall statistics\n",
    "        f.write(\"=== PREDICTION STATISTICS ===\\n\")\n",
    "        f.write(f\"Total predictions: {results['total_predictions']}\\n\")\n",
    "        f.write(f\"Legal moves: {results['legal_moves']} ({results['legal_rate']:.2f}%)\\n\")\n",
    "        f.write(f\"Illegal moves: {results['illegal_moves']} ({results['illegal_rate']:.2f}%)\\n\")\n",
    "        f.write(f\"Invalid format: {results['invalid_format']} ({results['invalid_rate']:.2f}%)\\n\\n\")\n",
    "        \n",
    "        # Write statistics by sequence length\n",
    "        f.write(\"=== DETAIL BY SEQUENCE LENGTH ===\\n\")\n",
    "        for length, stats in sorted(results['stats_by_length'].items()):\n",
    "            if stats['total'] > 0:\n",
    "                legal_pct = stats['legal'] / stats['total'] * 100\n",
    "                f.write(f\"Length {length}: {stats['legal']}/{stats['total']} legal moves ({legal_pct:.2f}%)\\n\")\n",
    "        \n",
    "        # Write timestamp\n",
    "        import datetime\n",
    "        f.write(f\"\\nResults generated on: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "For the Chess-bot will use a zeroShot method as it seem to be more consistent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
