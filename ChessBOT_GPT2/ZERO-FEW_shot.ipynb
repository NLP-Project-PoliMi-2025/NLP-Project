{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferdi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "#Standard GPT2: Tokenizer and model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Fine-tuned GPT2: Tokenizer1 and model1\n",
    "\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"Model90K\\\\model\")\n",
    "if tokenizer1.pad_token is None:\n",
    "    tokenizer1.pad_token = tokenizer1.eos_token\n",
    "\n",
    "model1 = GPT2LMHeadModel.from_pretrained(\"Model90K\\\\model\", trust_remote_code=True)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model,tokenizer,prompt, max_new_tokens=100, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    prompt_length = len(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],  \n",
    "            max_length=prompt_length + max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    new_text = generated_text[len(tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)):]\n",
    "    \n",
    "    return new_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "## if starting_fen is None, the board will be initialized to the starting position\n",
    "def get_fen_from_uci_sequence(uci_sequence, starting_fen=None):\n",
    "\n",
    "    \n",
    "    if starting_fen:\n",
    "        board = chess.Board(starting_fen)\n",
    "    else:\n",
    "        board = chess.Board()  \n",
    "    \n",
    "    moves = uci_sequence.strip().split()\n",
    "    \n",
    "    for move_str in moves:\n",
    "        \n",
    "        try:\n",
    "            move = chess.Move.from_uci(move_str)\n",
    "            \n",
    "            if move in board.legal_moves:\n",
    "                board.push(move)\n",
    "\n",
    "            else:\n",
    "                print(f\"Moves {move_str} not legal!\")\n",
    "                print(\"Moves sequence was interrupted.\")\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(f\"Moves {move_str} not in UCI format!\")\n",
    "            print(\"Moves sequence was interrupted.\")\n",
    "            break\n",
    "    \n",
    "    # Stampa la scacchiera nello stato attuale\n",
    "    print(\"\\nState of the board:\")\n",
    "    print(board)\n",
    "    \n",
    "    # Ritorna la FEN finale\n",
    "    final_fen = board.fen()\n",
    "    print(f\"\\nFEN: {final_fen}\")\n",
    "    \n",
    "    return final_fen, board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of the Hystory of the Game from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State of the board:\n",
      "r n . q k . . r\n",
      "p p . . p p B p\n",
      ". . . p . n p .\n",
      ". . p . . . . .\n",
      ". . P N . . b .\n",
      ". . . P . . . .\n",
      "P P . . P P P P\n",
      "R . . Q K B N R\n",
      "\n",
      "FEN: rn1qk2r/pp2ppBp/3p1np1/2p5/2PN2b1/3P4/PP2PPPP/R2QKBNR b KQkq - 2 7\n"
     ]
    }
   ],
   "source": [
    "movesHistory=\"c2c4 c7c5 b1a3 g8f6 d2d3 d7d6 a3b5 g7g6 c1h6 f8g7 h6g7 c8g4 b5d4\"\n",
    "\n",
    "FEN,board=get_fen_from_uci_sequence(movesHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f\"\"\"You are a chess assistant. Given a sequence of UCI moves, suggest two reasonable next moves for the side to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "\n",
    "Moves so far:\n",
    "{movesHistory}\n",
    "\n",
    "Your suggestions:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "Move #1:\n",
      "\n",
      "4. d4 dxe4 5. Nf2\n",
      "\n",
      "Move #2:\n",
      "\n",
      "5. Bc4\n",
      "\n",
      "Move #3:\n",
      "\n",
      "6. b5 e6 7. B\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "g4d1 d1c2 d6d5 c4d5 f6d5 g7e5 d8c7 e5d7 e7e6 d5e6 d7e8 e6f7 e8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,prompt1,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,prompt1,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of the prompt asking for 2 possibile moves got completely ignored, the finetuned model simply start trying playing for both the player.\n",
    "The slice of the prompt: \"Remeber the first moves of the sequence I gave is played by white, the second by black and so on\" is crucial for the second model otherwise the response often start with a moves of the wrong player\n",
    "\n",
    "The difference between the two model is clear stdGPT2 doesn't get what is a UCI format by itself and as a single move show some random char, on the other hand during the other model seem not to understand anymore the request of having to suggestion.\n",
    "\n",
    "Sometimes the finetuned model start the response with a single char followed by a space and the a sequence of UCI moves, if the single CHAR is ignored the following move is usually legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State of the board:\n",
      ". . . . . k . .\n",
      ". p . R . p p p\n",
      "p . . b . . . .\n",
      ". . . . . . Q .\n",
      ". . . . . P . .\n",
      ". . . . . . . P\n",
      "P . . . . K P .\n",
      ". . . q . . . .\n",
      "\n",
      "FEN: 5k2/1p1R1ppp/p2b4/6Q1/5P2/7P/P4KP1/3q4 b - - 27 43\n"
     ]
    }
   ],
   "source": [
    "movesHistoryLong=\"d2d4 d7d5 c2c4 e7e6 b1c3 c7c5 c4d5 e6d5 g1f3 g8f6 c1g5 c5d4 f3d4 f8e7 e2e3 b8c6 f1e2 e8g8 e1g1 a7a6 a1c1 f8e8 e2f3 c8e6 c3e2 c6e5 e2f4 e5f3 d1f3 e6g4 f3g3 d8d7 g5f6 e7f6 h2h3 g4f5 d4f5 d7f5 c1c5 a8d8 f1d1 f6b2 c5d5 d8d5 f4d5 f5c2 d5e7 g8f8 d1d7 c2c5 e7d5 b2e5 f2f4 e5d6 g3g5 e8e3 d5e3 c5e3 g1f1 e3d3 f1f2 d3d4 f2f3 d4d1 f3f2 d1d2 f2f1 d2d1 f1f2 d1d2 f2f1 d2d3 f1f2 d3d2 f2f1 d2d3 f1f2 d3d2 f2f1 d2d3 f1f2 d3d4 f2f3 d4d1 f3f2\"\n",
    "\n",
    "FEN,board=get_fen_from_uci_sequence(movesHistoryLong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptL = f\"\"\"You are a chess assistant. Given a sequence of UCI moves, suggest two reasonable next moves for the side to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "\n",
    "Moves so far:\n",
    "{movesHistoryLong}\n",
    "\n",
    "Your suggestions:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "This is the most reasonable move, given the sequence I gave.\n",
      "\n",
      "The last move is played by white, the second by black and so on.\n",
      "\n",
      "Remeber the first moves of the sequence I gave is played by white\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "1 f2d3 f1 f1f2 d3d2 f2f3 d1f2 d3d4 f3d2 f2 d7d5 f2 f2f3 d3d2 f1 f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,promptL,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,promptL,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already with an history not huge compared to the one in the dataset also the fineTuned model start messing up proposing unlegal moves,sometimes moving a piece like it was another, and also start showing \"UCI moves\" of 2 char; that probably caused by the short context GPT2 can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt2=f\"\"\"\n",
    "You are a chess assistant. Given a sequence of UCI moves, suggest two plausible next moves for the player to play.\n",
    "Remeber the first moves of the sequence I gave is played by white, the second by black and so on.\n",
    "    \n",
    "Example 1  \n",
    "Moves so far:  \n",
    "d2d4 g8f6 c2c4 e7e6 g1f3 d7d5 b1c3 f8e7 c1g5 h7h6  \n",
    "Suggestions: g5h4 e2e3  \n",
    "\n",
    "Example 2  \n",
    "Moves so far:  \n",
    "e2e4 e7e5 g1f3 b8c6 f1b5 a7a6 b5a4 g8f6 e1g1 f8e7 f1e1 b7b5 a4b3  \n",
    "Suggestions: c2c3 h2h3  \n",
    "\n",
    "Now it's your turn:\n",
    "\n",
    "Moves so far:  \n",
    "{movesHistory}  \n",
    "Suggestions: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GPT2 response:\n",
      "Example 3  \n",
      "\n",
      "Moves so far:  \n",
      "\n",
      "e2e4 e7e5  \n",
      "\n",
      "Ditto.\n",
      "\n",
      "Moves so far:  \n",
      "\n",
      "e2e4 e7e\n",
      "\n",
      "Fine-tuned GPT2 response:\n",
      "1c1 d8e8 g7f5 f5e4 d3e2 e2a6 a2a4 e4f5 g4f3 g6f5 e7d7 a6b7 b7c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdGPT2Response=generate_text(model,tokenizer,prompt2,50)\n",
    "print(f\"Standard GPT2 response:\\n{stdGPT2Response}\\n\")\n",
    "\n",
    "fineTuneGPT2Response=generate_text(model1,tokenizer1,prompt2,50)\n",
    "print(f\"Fine-tuned GPT2 response:\\n{fineTuneGPT2Response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems seen with GPT2 stay the same, also the thing it say are not coherent with the movesHistory: \"c2c4 c7c5 b1a3 g8f6 d2d3 d7d6 a3b5 g7g6 c1h6 f8g7 h6g7 c8g4 b5d4\"\n",
    "\n",
    "The fineTuned model here doesn't improve in any way and more often start the sequence with a 3 char which doesn't represent any moves in the UCI format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "For the Chess-bot will use a zeroShot method as it seem to be more consistent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
